# Manuscript assets

This document describes the manuscript assets (figures and tables) generated for the LLM data extraction paper.

## Overview

The manuscript assets are generated by the script:

```
scripts/python/analysis/create-manuscript-assets.py
```

Three assets are produced:

- Figure 1: Heatmap visualization of LLM performance with gruvbox dark colorscheme
- Table 1: Comprehensive model performance by extraction group (CSV and LaTeX)
- Table 2: Validation and data quality metrics (CSV and LaTeX)

## Generated assets

### Figure 1: LLM performance heatmap

**Location**: `data/artifacts/manuscript-figures/llm-performance.png`

**Description**: Heatmap showing mean assessment scores (scale 0-10) for 8 LLMs evaluated across 5 extraction groups and 3 assessment dimensions.

**Heatmap value calculation**:

Each cell in the heatmap represents the mean score for a specific model, extraction group, and dimension combination. The values are calculated as follows:

1. **Assessment structure**: For each extraction group (Q-1 through Q-5), reviewers assessed three dimensions:
   - **Accuracy** (Q-a): Evaluated using questions Q-a-1, Q-a-2, and Q-a-3
     - Q-a-1: Total number of results reported by model
     - Q-a-2: Number of results reported incorrectly
     - Q-a-3: Overall assessment of accuracy (score 1-10)
   - **Detail** (Q-b): Evaluated using questions Q-b-1, Q-b-2, and Q-b-3
     - Q-b-1: Number of results reported with sufficient detail
     - Q-b-2: Overall assessment of appropriate detail (score 1-10)
     - Q-b-3: Free text description of issues
   - **Completeness** (Q-c): Evaluated using questions Q-c-1, Q-c-2, and Q-c-3
     - Q-c-1: Number of results not reported by model
     - Q-c-2: Overall assessment of missing information (score 1-10)
     - Q-c-3: Free text description of results not reported

2. **Score extraction**: For the heatmap, only the overall assessment scores are used:
   - Accuracy: Q-a-3 (column suffix `a-3`)
   - Detail: Q-b-2 (column suffix `b-2`)
   - Completeness: Q-c-2 (column suffix `c-2`)

3. **Aggregation**: For each model and metric combination:
   - Extract all scores for that metric across 100 papers and 2 reviewers (200 assessments total)
   - Filter out zero scores (indicating not assessed)
   - Calculate the mean of all valid scores (scores > 0)
   - Round to 1 decimal place for display

4. **Example**: The value in cell `gpt-4-1 × Q-1-Accuracy` represents:
   - Mean of all Q-1-a-3 scores for gpt-4-1
   - Based on up to 200 assessments (100 papers × 2 reviewers)
   - Excluding any zero/missing scores

**Extraction groups**:

- Q-1: Exposure Traits
- Q-2: Outcome Traits
- Q-3: Analytical Methods
- Q-4: Populations
- Q-5: Reported Results

**Structure**:

- Rows: 8 models (ordered by overall performance)
- Columns: 15 metrics organized by extraction group
  - Q-1 (Exposure Traits): Accuracy, Detail, Completeness
  - Q-2 (Outcome Traits): Accuracy, Detail, Completeness
  - Q-3 (Methods): Accuracy, Detail, Completeness
  - Q-4 (Populations): Accuracy, Detail, Completeness
  - Q-5 (Results): Accuracy, Detail, Completeness

**Visual encoding**:

- Color scheme: Gruvbox dark colormap (dark to light gradient)
- Scale: 0-10 score range
- Annotations: Score values displayed in each cell

**Key findings**:

- All models achieved high accuracy (9.5-9.9) and detail (9.4-9.9) in assessments
- Completeness varied more (4.6-5.7)
- Populations (Q-4) and Results (Q-5) showed lowest completeness across all models

### Table 1: Comprehensive model performance

**Locations**:
- CSV: `data/artifacts/manuscript-tables/llm-extraction-performance-detail.csv`
- LaTeX: `data/artifacts/manuscript-tables/llm-extraction-performance-detail.tex`

**Description**: Complete quantitative data showing model performance across all extraction groups and dimensions.

**Columns**:

- `model`: Model name
- `family`: Model family (OpenAI, Local)
- `overall_mean`: Overall mean score across all metrics
- `overall_sd`: Standard deviation of all scores
- Group-specific metrics (15 columns):
  - `Q-1-Accuracy`, `Q-1-Detail`, `Q-1-Completeness`
  - `Q-2-Accuracy`, `Q-2-Detail`, `Q-2-Completeness`
  - `Q-3-Accuracy`, `Q-3-Detail`, `Q-3-Completeness`
  - `Q-4-Accuracy`, `Q-4-Detail`, `Q-4-Completeness`
  - `Q-5-Accuracy`, `Q-5-Detail`, `Q-5-Completeness`

**Rows**: 8 models ordered by overall performance (descending)

**Key features**:

- All numeric values rounded to 2 decimal places
- Models grouped by family for easy comparison
- LaTeX table uses booktabs formatting

### Table 2: Validation and data quality metrics

**Locations**:
- CSV: `data/artifacts/manuscript-tables/llm-validation-metrics.csv`
- LaTeX: `data/artifacts/manuscript-tables/llm-validation-metrics.tex`

**Description**: Validation metrics documenting schema compliance and data quality issues that informed production model selection.

**Columns**:

- `model`: Model name
- `sample_size`: Number of papers in the evaluation sample
- `total_extractions`: Total number of extraction attempts
- `valid_extractions`: Number passing schema validation
- `invalid_extractions`: Number failing schema validation
- `validation_issue_rate`: Percentage of extractions with validation issues

**Rows**: 7 models ordered by validation issue rate (ascending)

**Sample sizes**:

- 15635 papers: gpt-5, gpt-4-1 (full dataset)
- 7000 papers: All other models (pilot sample)

**Key features**:

- All numeric values rounded to 1 decimal place
- LaTeX table uses booktabs formatting

**Key findings**:

- gpt-5 and gpt-4-1 have the lowest validation issue rates (0.0%)
- Local models show variable validation performance (0.7% to 88.4%)
- This explains production model selection despite similar assessment performance scores

## Data sources

### Assessment data

**Source**: `data/artifacts/assessment-results/assessment-results-numeric.csv`

**Origin**: Processed from independent reviewer assessments using `scripts/python/analysis/process-assessment-data.py`

**Content**: Numeric scores (1-10 scale) from 2 reviewers evaluating 100 papers across 8 models

**Structure**:

- Rows: 1,600 assessments (100 papers × 8 models × 2 reviewers)
- Columns: pmid, model, and assessment scores organized by:
  - 5 extraction groups (Q-1 to Q-5)
  - 3 dimensions per group (accuracy, detail, completeness)
  - Multiple sub-questions per dimension

**Column naming convention**:

For each extraction group (Q-1 through Q-5), columns follow the pattern:
- `Q-X-a-1`, `Q-X-a-2`, `Q-X-a-3`: Accuracy questions
- `Q-X-b-1`, `Q-X-b-2`, `Q-X-b-3`: Detail questions
- `Q-X-c-1`, `Q-X-c-2`, `Q-X-c-3`: Completeness questions

The heatmap uses the overall assessment scores (Q-X-a-3, Q-X-b-2, Q-X-c-2) for each dimension.

### Validation metrics

**Source**: `data/intermediate/llm-results-aggregated/<model>/`

**Files per model**:

- `processed_results.json`: All processed extractions
- `processed_results_valid.json`: Extractions passing schema validation
- `processed_results_invalid.json`: Extractions failing schema validation

**Schema location**: `data/assets/data-schema/processed_results/`

**Validation process**: Performed by `scripts/python/postprocessing/process-llm-aggregated-results.py`

## Usage

### Regenerating assets

To regenerate all assets:

```bash
python scripts/python/analysis/create-manuscript-assets.py --verbose
```

Options:

- `--assessment-file`: Path to assessment results CSV (default: auto-detected)
- `--aggregated-dir`: Directory with aggregated LLM results (default: auto-detected)
- `--output-figures`: Output directory for figures (default: data/artifacts/manuscript-figures)
- `--output-tables`: Output directory for tables (default: data/artifacts/manuscript-tables)
- `-n / --dry-run`: Preview without generating outputs
- `-v / --verbose`: Print detailed progress information

### Modifying visualizations

The figure uses matplotlib and seaborn with a gruvbox dark colorscheme. To customize:

1. Edit `create_combined_figure()` in `create-manuscript-assets.py`
2. Modify color schemes (change `gruvbox_colors` list), figure dimensions, or metrics
3. Regenerate with `--verbose` flag to see changes

### Exporting for publication

**PNG figure** (`llm-performance.png`):
- Publication-ready at 300 DPI
- Can be inserted directly into manuscripts
- Can be converted to other formats (PDF, EPS) using standard tools

**CSV tables**:
- Open in Excel or similar spreadsheet software
- Format for publication manually if needed

**LaTeX tables**:
- Ready to include in LaTeX documents using `\input{filename.tex}`
- Uses booktabs package formatting (`\toprule`, `\midrule`, `\bottomrule`)
- May need to wrap in `\begin{table}...\end{table}` environment and add caption

## Notes

### Model naming

- gpt-5-mini data not included in current validation metrics (directory not found)
- deepseek-r1-distilled: Full name for DeepSeek R1 Distilled model

### Validation issues

High validation issue rates (e.g., gpt-4o at 100%, deepseek-r1-distilled at 88.4%) indicate:

- Incorrect JSON structure
- Missing required fields
- Type mismatches
- Out-of-schema content

These issues prevent downstream use despite potentially good subjective performance scores.

### Assessment scoring

**Scoring scale**: All assessment scores use a 1-10 scale:

- 10: Perfect/Complete
- 1: Very poor/Minimal
- 0: Not assessed (excluded from analysis)

**Assessment methodology**:

Each of the 100 papers was independently assessed by 2 reviewers for each of the 8 models, resulting in 1,600 total assessments (100 papers × 8 models × 2 reviewers).

For each extraction group (Q-1 through Q-5), reviewers provided:
1. Quantitative counts (number of results, errors, missing items)
2. Overall assessment scores (1-10 scale) for Accuracy, Detail, and Completeness
3. Free text descriptions of issues

**Score aggregation**:

The heatmap and tables display mean scores calculated by:
1. Collecting all valid scores (> 0) for each model and metric
2. Computing the arithmetic mean across all assessments
3. Excluding zero scores which indicate items not assessed

For each cell in the heatmap:
- Maximum possible assessments: 200 (100 papers × 2 reviewers)
- Actual assessments: Variable (excluding zero scores)
- Displayed value: Mean of all valid assessment scores

## Related documentation

- @DATA.md: Overall data organization
- @ANALYSIS.md: Analysis workflow and procedures
- Assessment data processing: `scripts/python/analysis/process-assessment-data.py`
- Validation and processing: `scripts/python/postprocessing/process-llm-aggregated-results.py`
