{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract mr pubmed abstracts data using OpenAI models\n",
    "\n",
    "NOTE: this should be part of a slurm job submission\n",
    "\n",
    "Logics\n",
    "- Processes abstracts in batches based on SLURM array task ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from environs import env\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "from local_funcs import openai_funcs, prompt_funcs\n",
    "from yiutils.chunking import calculate_chunk_start_end\n",
    "from yiutils.project_utils import find_project_root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== params ====\n",
    "PROJECT_ROOT = find_project_root(\"justfile\")\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "PATH_DATA = DATA_DIR / \"intermediate\" / \"mr-pubmed-data\" / \"mr-pubmed-data-sample.json\"\n",
    "PATH_SCHEMA_DIR = DATA_DIR / \"assets\" / \"data-schema\" / \"example-data\"\n",
    "\n",
    "MODEL_CONFIGS = {\n",
    "    \"o4-mini\": {\"model_id\": \"o4-mini\", \"chat_func\": openai_funcs.get_o4_mini_result},\n",
    "    \"gpt-4o\": {\"model_id\": \"gpt-4o\", \"chat_func\": openai_funcs.get_gpt_4o_result},\n",
    "}\n",
    "PILOT_NUM_DOCS = 20\n",
    "ARRAY_LENGTH = 30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Argument Parsing (for notebook, set variables directly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set these variables directly in the notebook\n",
    "output_dir = PROJECT_ROOT / \"output\"\n",
    "path_data = PATH_DATA\n",
    "array_id = 0\n",
    "pilot = True\n",
    "model = \"o4-mini\"  # or \"gpt-4o\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Starting path not found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m array_task_id = array_id\n\u001b[32m      3\u001b[39m openai_api_key = env(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/data-extraction-non-gpu/lib/python3.12/site-packages/environs/__init__.py:409\u001b[39m, in \u001b[36mEnv.read_env\u001b[39m\u001b[34m(path, recurse, verbose, override, return_path)\u001b[39m\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m start_dir:  \u001b[38;5;66;03m# Only a filename was given\u001b[39;00m\n\u001b[32m    408\u001b[39m     start_dir = os.getcwd()\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdirname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_walk_to_root\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheck_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_name\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcheck_path\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/data-extraction-non-gpu/lib/python3.12/site-packages/dotenv/main.py:263\u001b[39m, in \u001b[36m_walk_to_root\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    259\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[33;03mYield directories starting from the given directory up to the root\u001b[39;00m\n\u001b[32m    261\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(path):\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mStarting path not found\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.isfile(path):\n\u001b[32m    266\u001b[39m     path = os.path.dirname(path)\n",
      "\u001b[31mOSError\u001b[39m: Starting path not found"
     ]
    }
   ],
   "source": [
    "env.read_env()\n",
    "array_task_id = array_id\n",
    "openai_api_key = env(\"OPENAI_API_KEY\")\n",
    "pilot_num_docs = PILOT_NUM_DOCS\n",
    "array_length = ARRAY_LENGTH\n",
    "\n",
    "# ==== chunking data ====\n",
    "path_to_pubmed = Path(path_data)\n",
    "assert path_data is not None and path_to_pubmed.exists(), \"pubmed data not found\"\n",
    "\n",
    "# Load data length for correct startpoint/endpoint calculation\n",
    "print(f\"Loading data from {path_to_pubmed}\")\n",
    "with path_to_pubmed.open(\"r\") as f:\n",
    "    pubmed_data = json.load(f)\n",
    "data_length = len(pubmed_data)\n",
    "\n",
    "# Use calculate_start_end to determine startpoint and endpoint\n",
    "startpoint, endpoint = calculate_chunk_start_end(\n",
    "    chunk_id=array_task_id,\n",
    "    num_chunks=array_length,\n",
    "    data_length=data_length,\n",
    "    pilot_num_docs=pilot_num_docs,\n",
    "    pilot=pilot,\n",
    "    verbose=True,\n",
    ")\n",
    "if startpoint is None or endpoint is None:\n",
    "    print(f\"WARNING: startpoint {startpoint} endpoint {endpoint}\")\n",
    "    raise RuntimeError(\"Invalid startpoint/endpoint\")\n",
    "pubmed_data = pubmed_data[startpoint:endpoint]\n",
    "\n",
    "# ==== model config ====\n",
    "model_config_name = model\n",
    "assert model_config_name is not None and model_config_name in MODEL_CONFIGS.keys(), (\n",
    "    \"--model must not be empty and must be one of the configs\"\n",
    ")\n",
    "model_config = MODEL_CONFIGS[model_config_name]\n",
    "\n",
    "# ==== output directory and file ====\n",
    "output_dir = Path(output_dir) / model_config_name\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "pilot_suffix = \"_pilot\" if pilot else \"\"\n",
    "out_file = output_dir / f\"mr_extract_openai_array_{array_task_id}{pilot_suffix}.json\"\n",
    "\n",
    "print(f\"Loaded {len(pubmed_data)} abstracts from {path_to_pubmed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_openai_client(api_key):\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    print(\"Loaded OpenAI client\")\n",
    "    return client\n",
    "\n",
    "\n",
    "client = setup_openai_client(api_key=openai_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_schema_data():\n",
    "    schema_config = {\n",
    "        \"metadata\": {\n",
    "            \"example\": PATH_SCHEMA_DIR / \"metadata.json\",\n",
    "            \"schema\": PATH_SCHEMA_DIR / \"metadata.schema.json\",\n",
    "        },\n",
    "        \"results\": {\n",
    "            \"example\": PATH_SCHEMA_DIR / \"results.json\",\n",
    "            \"schema\": PATH_SCHEMA_DIR / \"results.schema.json\",\n",
    "        },\n",
    "    }\n",
    "    # Check if the four schema files exist\n",
    "    missing_files = []\n",
    "    schema_data = {}\n",
    "    for section_name, section in schema_config.items():\n",
    "        schema_data[section_name] = {}\n",
    "        for key, path in section.items():\n",
    "            if not path.exists():\n",
    "                missing_files.append(str(path))\n",
    "                schema_data[section_name][key] = None\n",
    "            else:\n",
    "                with path.open(\"r\") as f:\n",
    "                    try:\n",
    "                        schema_data[section_name][key] = json.load(f)\n",
    "                    except Exception as e:\n",
    "                        print(f\"ERROR loading {path}: {e}\")\n",
    "                        schema_data[section_name][key] = None\n",
    "    if missing_files:\n",
    "        print(f\"WARNING: The following schema files do not exist: {missing_files}\")\n",
    "    else:\n",
    "        print(\"All schema files found.\")\n",
    "    return schema_data\n",
    "\n",
    "\n",
    "schema_data = load_schema_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_abstract(article_data, schema_data, client, model_config):\n",
    "    try:\n",
    "        chat_func = model_config[\"chat_func\"]\n",
    "        input_prompt_metadata = prompt_funcs.make_message_metadata_new(\n",
    "            abstract=article_data[\"ab\"],\n",
    "            json_example=schema_data[\"metadata\"][\"example\"],\n",
    "            json_schema=schema_data[\"metadata\"][\"schema\"],\n",
    "        )\n",
    "        input_prompt_results = prompt_funcs.make_message_metadata_new(\n",
    "            abstract=article_data[\"ab\"],\n",
    "            json_example=schema_data[\"results\"][\"example\"],\n",
    "            json_schema=schema_data[\"results\"][\"schema\"],\n",
    "        )\n",
    "        completion_metadata = chat_func(client, input_prompt_metadata)\n",
    "        completion_results = chat_func(client, input_prompt_results)\n",
    "        result = {\n",
    "            \"completion_metadata\": completion_metadata,\n",
    "            \"completion_results\": completion_results,\n",
    "        }\n",
    "        output = dict(article_data, **result)\n",
    "        return output\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\n=========== {article_data.get('pmid', 'NO PMID')} ===========\")\n",
    "        print(\"\\n=========== FAILED! ===========\")\n",
    "        print(e)\n",
    "        result1 = {\"metadata\": {}, \"metainformation\": {\"error\": f\"Failed {e}\"}}\n",
    "        output = dict(article_data, **result1)\n",
    "        print(f\"Output: {output}\")\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== process abstracts ====\n",
    "fulldata = []\n",
    "for article_data in tqdm(pubmed_data):\n",
    "    output = process_abstract(\n",
    "        article_data=article_data,\n",
    "        schema_data=schema_data,\n",
    "        client=client,\n",
    "        model_config=model_config,\n",
    "    )\n",
    "    fulldata.append(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== save output ====\n",
    "print(f\"Wrote results to {out_file}\")\n",
    "with out_file.open(\"w\") as f:\n",
    "    json.dump(fulldata, f, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
