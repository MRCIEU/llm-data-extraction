{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune Gemma3 270M for MR Abstract Data Extraction\n",
    "\n",
    "This notebook fine-tunes a Gemma3 270M model using Unsloth to replicate the OpenAI-based extraction logic for Mendelian randomization abstracts.\n",
    "\n",
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\n",
    "!pip install --no-deps \"xformers<0.0.27\" trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset as HFDataset\n",
    "\n",
    "# Add local packages to path\n",
    "sys.path.insert(0, \"../../src/local_funcs/src\")\n",
    "sys.path.insert(0, \"../../src/yiutils/src\")\n",
    "\n",
    "from local_funcs import prompt_funcs, schema_funcs\n",
    "from yiutils.failsafe import safe_json_loads\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning configuration\n",
    "CONFIG = {\n",
    "    \"model_name\": \"unsloth/Gemma-2-2b\",  # Will use 270M when available\n",
    "    \"max_seq_length\": 2048,\n",
    "    \"dtype\": \"bfloat16\",\n",
    "    \"load_in_4bit\": True,\n",
    "    \n",
    "    # LoRA config\n",
    "    \"r\": 16,\n",
    "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0.0,\n",
    "    \"bias\": \"none\",\n",
    "    \"use_gradient_checkpointing\": \"unsloth\",\n",
    "    \"random_state\": 3407,\n",
    "    \n",
    "    # Training config\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"warmup_steps\": 5,\n",
    "    \"max_steps\": 60,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"fp16\": False,\n",
    "    \"bf16\": True,\n",
    "    \"logging_steps\": 1,\n",
    "    \"optim\": \"adamw_8bit\",\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"lr_scheduler_type\": \"linear\",\n",
    "    \"seed\": 3407,\n",
    "}\n",
    "\n",
    "print(f\"Configuration: {CONFIG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Load existing OpenAI extraction results to create training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(data_dir: Path) -> Tuple[List[str], List[str], List[str]]:\n",
    "    \"\"\"Load training data from existing OpenAI extraction results.\"\"\"\n",
    "    logger.info(f\"Loading training data from {data_dir}\")\n",
    "    \n",
    "    abstracts = []\n",
    "    metadata_extractions = []\n",
    "    results_extractions = []\n",
    "    \n",
    "    # Look for processed results files\n",
    "    for result_file in data_dir.glob(\"**/*results*.json\"):\n",
    "        try:\n",
    "            with open(result_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Extract abstracts and extractions from OpenAI results\n",
    "            if isinstance(data, list):\n",
    "                for item in data:\n",
    "                    if 'abstract' in item and 'extracted_metadata' in item and 'extracted_results' in item:\n",
    "                        abstracts.append(item['abstract'])\n",
    "                        metadata_extractions.append(json.dumps(item['extracted_metadata']))\n",
    "                        results_extractions.append(json.dumps(item['extracted_results']))\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to load {result_file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    logger.info(f\"Loaded {len(abstracts)} training examples\")\n",
    "    return abstracts, metadata_extractions, results_extractions\n",
    "\n",
    "\n",
    "# Load data\n",
    "data_dir = Path(\"../../data/intermediate\")\n",
    "abstracts, metadata_extractions, results_extractions = load_training_data(data_dir)\n",
    "\n",
    "print(f\"Found {len(abstracts)} training examples\")\n",
    "if len(abstracts) > 0:\n",
    "    print(f\"Example abstract: {abstracts[0][:200]}...\")\n",
    "    print(f\"Example metadata: {metadata_extractions[0][:200]}...\")\n",
    "else:\n",
    "    print(\"No training data found. Please run OpenAI extraction first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_examples(abstracts: List[str], metadata_extractions: List[str], \n",
    "                            results_extractions: List[str]) -> List[Dict[str, str]]:\n",
    "    \"\"\"Prepare training examples in instruction format.\"\"\"\n",
    "    examples = []\n",
    "    \n",
    "    for abstract, metadata, results in zip(abstracts, metadata_extractions, results_extractions):\n",
    "        # Metadata extraction example\n",
    "        metadata_instruction = (\n",
    "            \"Extract metadata from the following Mendelian randomization abstract. \"\n",
    "            \"Return valid JSON with fields: title, authors, journal, year, pmid, doi, abstract.\\n\\n\"\n",
    "            f\"Abstract: {abstract}\\n\\n\"\n",
    "            \"Metadata JSON:\"\n",
    "        )\n",
    "        \n",
    "        examples.append({\n",
    "            \"instruction\": metadata_instruction,\n",
    "            \"output\": metadata,\n",
    "        })\n",
    "        \n",
    "        # Results extraction example\n",
    "        results_instruction = (\n",
    "            \"Extract MR results from the following Mendelian randomization abstract. \"\n",
    "            \"Return valid JSON with fields: exposures, outcomes, mr_methods, effect_estimates, \"\n",
    "            \"pvalues, confidence_intervals, sample_sizes, populations, study_design.\\n\\n\"\n",
    "            f\"Abstract: {abstract}\\n\\n\"\n",
    "            \"Results JSON:\"\n",
    "        )\n",
    "        \n",
    "        examples.append({\n",
    "            \"instruction\": results_instruction,\n",
    "            \"output\": results,\n",
    "        })\n",
    "    \n",
    "    return examples\n",
    "\n",
    "\n",
    "def format_prompts(examples: List[Dict[str, str]]) -> List[str]:\n",
    "    \"\"\"Format examples as conversation prompts.\"\"\"\n",
    "    formatted = []\n",
    "    \n",
    "    for example in examples:\n",
    "        # Use Gemma format\n",
    "        prompt = f\"<bos><start_of_turn>user\\n{example['instruction']}<end_of_turn>\\n<start_of_turn>model\\n{example['output']}<eos>\"\n",
    "        formatted.append(prompt)\n",
    "    \n",
    "    return formatted\n",
    "\n",
    "\n",
    "# Prepare training examples\n",
    "if len(abstracts) > 0:\n",
    "    examples = prepare_training_examples(abstracts, metadata_extractions, results_extractions)\n",
    "    formatted_prompts = format_prompts(examples)\n",
    "    \n",
    "    # Create HuggingFace dataset\n",
    "    train_dataset = HFDataset.from_dict({\"text\": formatted_prompts})\n",
    "    \n",
    "    print(f\"Prepared {len(formatted_prompts)} training examples\")\n",
    "    print(f\"Example formatted prompt: {formatted_prompts[0][:300]}...\")\n",
    "else:\n",
    "    print(\"Skipping training data preparation - no data found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading\n",
    "\n",
    "Load Gemma3 270M with Unsloth for efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "def load_unsloth_model(config: dict):\n",
    "    \"\"\"Load Gemma3 270M model with Unsloth.\"\"\"\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=config[\"model_name\"],\n",
    "        max_seq_length=config[\"max_seq_length\"],\n",
    "        dtype=getattr(torch, config[\"dtype\"]) if hasattr(torch, config[\"dtype\"]) else None,\n",
    "        load_in_4bit=config[\"load_in_4bit\"],\n",
    "    )\n",
    "    \n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=config[\"r\"],\n",
    "        target_modules=config[\"target_modules\"],\n",
    "        lora_alpha=config[\"lora_alpha\"],\n",
    "        lora_dropout=config[\"lora_dropout\"],\n",
    "        bias=config[\"bias\"],\n",
    "        use_gradient_checkpointing=config[\"use_gradient_checkpointing\"],\n",
    "        random_state=config[\"random_state\"],\n",
    "        use_rslora=False,\n",
    "        loftq_config=None,\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "# Load model\n",
    "logger.info(\"Loading Unsloth model...\")\n",
    "model, tokenizer = load_unsloth_model(CONFIG)\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model: {model.__class__.__name__}\")\n",
    "print(f\"Tokenizer: {tokenizer.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "def create_trainer(model, tokenizer, train_dataset, config: dict):\n",
    "    \"\"\"Create trainer for fine-tuning.\"\"\"\n",
    "    training_args = TrainingArguments(\n",
    "        per_device_train_batch_size=config[\"per_device_train_batch_size\"],\n",
    "        gradient_accumulation_steps=config[\"gradient_accumulation_steps\"],\n",
    "        warmup_steps=config[\"warmup_steps\"],\n",
    "        max_steps=config[\"max_steps\"],\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        fp16=config[\"fp16\"],\n",
    "        bf16=config[\"bf16\"],\n",
    "        logging_steps=config[\"logging_steps\"],\n",
    "        optim=config[\"optim\"],\n",
    "        weight_decay=config[\"weight_decay\"],\n",
    "        lr_scheduler_type=config[\"lr_scheduler_type\"],\n",
    "        seed=config[\"seed\"],\n",
    "        output_dir=\"./gemma3-270m-mr-extraction\",\n",
    "    )\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=train_dataset,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=config[\"max_seq_length\"],\n",
    "        dataset_num_proc=2,\n",
    "        packing=False,\n",
    "        args=training_args,\n",
    "    )\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "\n",
    "# Create trainer\n",
    "if 'train_dataset' in locals():\n",
    "    trainer = create_trainer(model, tokenizer, train_dataset, CONFIG)\n",
    "    print(\"Trainer created successfully!\")\n",
    "else:\n",
    "    print(\"Skipping trainer creation - no training dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Start the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "if 'trainer' in locals():\n",
    "    logger.info(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    print(\"Training completed!\")\n",
    "else:\n",
    "    print(\"Skipping training - no trainer available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Test the fine-tuned model on sample abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, test_abstracts: List[str]):\n",
    "    \"\"\"Evaluate the fine-tuned model on test examples.\"\"\"\n",
    "    logger.info(\"Evaluating model...\")\n",
    "    \n",
    "    FastLanguageModel.for_inference(model)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, abstract in enumerate(test_abstracts[:3]):  # Test on first 3 examples\n",
    "        print(f\"\\nEvaluating example {i+1}/3...\")\n",
    "        \n",
    "        # Test metadata extraction\n",
    "        metadata_instruction = (\n",
    "            \"Extract metadata from the following Mendelian randomization abstract. \"\n",
    "            \"Return valid JSON with fields: title, authors, journal, year, pmid, doi, abstract.\\n\\n\"\n",
    "            f\"Abstract: {abstract}\\n\\n\"\n",
    "            \"Metadata JSON:\"\n",
    "        )\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            [f\"<bos><start_of_turn>user\\n{metadata_instruction}<end_of_turn>\\n<start_of_turn>model\\n\"],\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(\"cuda\")\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            use_cache=True,\n",
    "            temperature=0.1,\n",
    "            do_sample=True,\n",
    "        )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        metadata_extracted = response.split(\"<start_of_turn>model\\n\")[-1]\n",
    "        \n",
    "        print(f\"Metadata extraction: {metadata_extracted[:200]}...\")\n",
    "        \n",
    "        # Test results extraction\n",
    "        results_instruction = (\n",
    "            \"Extract MR results from the following Mendelian randomization abstract. \"\n",
    "            \"Return valid JSON with fields: exposures, outcomes, mr_methods, effect_estimates, \"\n",
    "            \"pvalues, confidence_intervals, sample_sizes, populations, study_design.\\n\\n\"\n",
    "            f\"Abstract: {abstract}\\n\\n\"\n",
    "            \"Results JSON:\"\n",
    "        )\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            [f\"<bos><start_of_turn>user\\n{results_instruction}<end_of_turn>\\n<start_of_turn>model\\n\"],\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(\"cuda\")\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            use_cache=True,\n",
    "            temperature=0.1,\n",
    "            do_sample=True,\n",
    "        )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        results_extracted = response.split(\"<start_of_turn>model\\n\")[-1]\n",
    "        \n",
    "        print(f\"Results extraction: {results_extracted[:200]}...\")\n",
    "        \n",
    "        results.append({\n",
    "            \"abstract\": abstract[:200] + \"...\",\n",
    "            \"metadata_extraction\": metadata_extracted,\n",
    "            \"results_extraction\": results_extracted,\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "if 'abstracts' in locals() and len(abstracts) > 0:\n",
    "    test_abstracts = abstracts[-5:]  # Use last 5 as test\n",
    "    evaluation_results = evaluate_model(model, tokenizer, test_abstracts)\n",
    "    \n",
    "    # Save results\n",
    "    output_dir = Path(\"./gemma3-270m-mr-extraction\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    with open(output_dir / \"evaluation_results.json\", \"w\") as f:\n",
    "        json.dump(evaluation_results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nEvaluation results saved to: {output_dir / 'evaluation_results.json'}\")\n",
    "else:\n",
    "    print(\"Skipping evaluation - no test data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Export\n",
    "\n",
    "Save the fine-tuned model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, tokenizer, output_dir: Path):\n",
    "    \"\"\"Save the fine-tuned model.\"\"\"\n",
    "    logger.info(f\"Saving model to {output_dir}\")\n",
    "    \n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    # Also save in GGUF format for inference\n",
    "    try:\n",
    "        model.save_pretrained_gguf(output_dir / \"gguf\", tokenizer, quantization_method=\"q4_k_m\")\n",
    "        logger.info(f\"Saved GGUF model to {output_dir / 'gguf'}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed to save GGUF model: {e}\")\n",
    "\n",
    "\n",
    "# Save model\n",
    "output_dir = Path(\"./gemma3-270m-mr-extraction\")\n",
    "save_model(model, tokenizer, output_dir)\n",
    "print(f\"Model saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Test\n",
    "\n",
    "Test the saved model with a sample abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference with a sample abstract\n",
    "sample_abstract = \"\"\"\n",
    "Background: The relationship between body mass index (BMI) and cardiovascular disease \n",
    "has been extensively studied, but the causal nature remains unclear. We used Mendelian \n",
    "randomization to investigate the causal effect of BMI on coronary artery disease.\n",
    "\n",
    "Methods: We used genetic variants associated with BMI as instrumental variables in a \n",
    "two-sample Mendelian randomization analysis. GWAS summary statistics were obtained \n",
    "from the GIANT consortium (n=339,224) for BMI and CARDIoGRAMplusC4D (n=184,305) \n",
    "for coronary artery disease.\n",
    "\n",
    "Results: Higher BMI was causally associated with increased risk of coronary artery \n",
    "disease (OR = 1.27, 95% CI: 1.15-1.40, P = 2.3e-6). The effect remained significant \n",
    "after adjustment for potential confounders.\n",
    "\n",
    "Conclusions: This study provides evidence for a causal relationship between higher \n",
    "BMI and increased coronary artery disease risk.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Testing inference on sample abstract...\")\n",
    "print(f\"Sample abstract: {sample_abstract[:200]}...\")\n",
    "\n",
    "# Test metadata extraction\n",
    "metadata_instruction = (\n",
    "    \"Extract metadata from the following Mendelian randomization abstract. \"\n",
    "    \"Return valid JSON with fields: title, authors, journal, year, pmid, doi, abstract.\\n\\n\"\n",
    "    f\"Abstract: {sample_abstract}\\n\\n\"\n",
    "    \"Metadata JSON:\"\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "inputs = tokenizer(\n",
    "    [f\"<bos><start_of_turn>user\\n{metadata_instruction}<end_of_turn>\\n<start_of_turn>model\\n\"],\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=512,\n",
    "    use_cache=True,\n",
    "    temperature=0.1,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "metadata_result = response.split(\"<start_of_turn>model\\n\")[-1]\n",
    "\n",
    "print(f\"\\nMetadata extraction result:\\n{metadata_result}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Fine-tuning completed successfully!\")\n",
    "print(f\"Model saved to: {output_dir}\")\n",
    "print(\"Use the saved model for inference on new MR abstracts.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
