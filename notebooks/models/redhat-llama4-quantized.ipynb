{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "743c40a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vllm'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLM, SamplingParams\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[32m      4\u001b[39m model_id = \u001b[33m\"\u001b[39m\u001b[33mRedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'vllm'"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic\"\n",
    "number_gpus = 2\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.7, top_p=0.8, max_tokens=256)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0a85cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b5a/ik18445.b5a/micromamba/envs/data-extraction/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.51.3\n",
      "2.6.0\n",
      "True\n",
      "12.6\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from environs import env\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    QuantoConfig,\n",
    "    AutoProcessor,\n",
    "    Llama4ForConditionalGeneration,\n",
    ")\n",
    "\n",
    "from local_funcs import chat_funcs, prompt_funcs\n",
    "from yiutils.project_utils import find_project_root\n",
    "\n",
    "proj_root = find_project_root(\"justfile\")\n",
    "data_dir = proj_root / \"data\"\n",
    "\n",
    "print(transformers.__version__)\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "\n",
    "env.read_env(proj_root / \".env\")\n",
    "access_token = env(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "path_to_mr_pubmed_data = (\n",
    "    data_dir / \"intermediate\" / \"mr-pubmed-data\" / \"mr-pubmed-data.json\"\n",
    ")\n",
    "assert path_to_mr_pubmed_data.exists(), (\n",
    "    f\"Data file {path_to_mr_pubmed_data} does not exist.\"\n",
    ")\n",
    "\n",
    "with open(path_to_mr_pubmed_data, \"r\") as f:\n",
    "    mr_pubmed_data = json.load(f)\n",
    "\n",
    "article_data = mr_pubmed_data[0]\n",
    "\n",
    "message_metadata = prompt_funcs.make_message_metadata(article_data[\"ab\"])\n",
    "message_results = prompt_funcs.make_message_results(article_data[\"ab\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09593b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 23 files: 100%|██████████| 23/23 [00:00<00:00, 237608.35it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 23/23 [00:04<00:00,  4.68it/s]\n",
      "Some weights of Llama4ForCausalLM were not initialized from the model checkpoint at RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic and are newly initialized: ['model.layers.0.feed_forward.experts.down_proj', 'model.layers.0.feed_forward.experts.gate_up_proj', 'model.layers.1.feed_forward.experts.down_proj', 'model.layers.1.feed_forward.experts.gate_up_proj', 'model.layers.10.feed_forward.experts.down_proj', 'model.layers.10.feed_forward.experts.gate_up_proj', 'model.layers.11.feed_forward.experts.down_proj', 'model.layers.11.feed_forward.experts.gate_up_proj', 'model.layers.12.feed_forward.experts.down_proj', 'model.layers.12.feed_forward.experts.gate_up_proj', 'model.layers.13.feed_forward.experts.down_proj', 'model.layers.13.feed_forward.experts.gate_up_proj', 'model.layers.14.feed_forward.experts.down_proj', 'model.layers.14.feed_forward.experts.gate_up_proj', 'model.layers.15.feed_forward.experts.down_proj', 'model.layers.15.feed_forward.experts.gate_up_proj', 'model.layers.16.feed_forward.experts.down_proj', 'model.layers.16.feed_forward.experts.gate_up_proj', 'model.layers.17.feed_forward.experts.down_proj', 'model.layers.17.feed_forward.experts.gate_up_proj', 'model.layers.18.feed_forward.experts.down_proj', 'model.layers.18.feed_forward.experts.gate_up_proj', 'model.layers.19.feed_forward.experts.down_proj', 'model.layers.19.feed_forward.experts.gate_up_proj', 'model.layers.2.feed_forward.experts.down_proj', 'model.layers.2.feed_forward.experts.gate_up_proj', 'model.layers.20.feed_forward.experts.down_proj', 'model.layers.20.feed_forward.experts.gate_up_proj', 'model.layers.21.feed_forward.experts.down_proj', 'model.layers.21.feed_forward.experts.gate_up_proj', 'model.layers.22.feed_forward.experts.down_proj', 'model.layers.22.feed_forward.experts.gate_up_proj', 'model.layers.23.feed_forward.experts.down_proj', 'model.layers.23.feed_forward.experts.gate_up_proj', 'model.layers.24.feed_forward.experts.down_proj', 'model.layers.24.feed_forward.experts.gate_up_proj', 'model.layers.25.feed_forward.experts.down_proj', 'model.layers.25.feed_forward.experts.gate_up_proj', 'model.layers.26.feed_forward.experts.down_proj', 'model.layers.26.feed_forward.experts.gate_up_proj', 'model.layers.27.feed_forward.experts.down_proj', 'model.layers.27.feed_forward.experts.gate_up_proj', 'model.layers.28.feed_forward.experts.down_proj', 'model.layers.28.feed_forward.experts.gate_up_proj', 'model.layers.29.feed_forward.experts.down_proj', 'model.layers.29.feed_forward.experts.gate_up_proj', 'model.layers.3.feed_forward.experts.down_proj', 'model.layers.3.feed_forward.experts.gate_up_proj', 'model.layers.30.feed_forward.experts.down_proj', 'model.layers.30.feed_forward.experts.gate_up_proj', 'model.layers.31.feed_forward.experts.down_proj', 'model.layers.31.feed_forward.experts.gate_up_proj', 'model.layers.32.feed_forward.experts.down_proj', 'model.layers.32.feed_forward.experts.gate_up_proj', 'model.layers.33.feed_forward.experts.down_proj', 'model.layers.33.feed_forward.experts.gate_up_proj', 'model.layers.34.feed_forward.experts.down_proj', 'model.layers.34.feed_forward.experts.gate_up_proj', 'model.layers.35.feed_forward.experts.down_proj', 'model.layers.35.feed_forward.experts.gate_up_proj', 'model.layers.36.feed_forward.experts.down_proj', 'model.layers.36.feed_forward.experts.gate_up_proj', 'model.layers.37.feed_forward.experts.down_proj', 'model.layers.37.feed_forward.experts.gate_up_proj', 'model.layers.38.feed_forward.experts.down_proj', 'model.layers.38.feed_forward.experts.gate_up_proj', 'model.layers.39.feed_forward.experts.down_proj', 'model.layers.39.feed_forward.experts.gate_up_proj', 'model.layers.4.feed_forward.experts.down_proj', 'model.layers.4.feed_forward.experts.gate_up_proj', 'model.layers.40.feed_forward.experts.down_proj', 'model.layers.40.feed_forward.experts.gate_up_proj', 'model.layers.41.feed_forward.experts.down_proj', 'model.layers.41.feed_forward.experts.gate_up_proj', 'model.layers.42.feed_forward.experts.down_proj', 'model.layers.42.feed_forward.experts.gate_up_proj', 'model.layers.43.feed_forward.experts.down_proj', 'model.layers.43.feed_forward.experts.gate_up_proj', 'model.layers.44.feed_forward.experts.down_proj', 'model.layers.44.feed_forward.experts.gate_up_proj', 'model.layers.45.feed_forward.experts.down_proj', 'model.layers.45.feed_forward.experts.gate_up_proj', 'model.layers.46.feed_forward.experts.down_proj', 'model.layers.46.feed_forward.experts.gate_up_proj', 'model.layers.47.feed_forward.experts.down_proj', 'model.layers.47.feed_forward.experts.gate_up_proj', 'model.layers.5.feed_forward.experts.down_proj', 'model.layers.5.feed_forward.experts.gate_up_proj', 'model.layers.6.feed_forward.experts.down_proj', 'model.layers.6.feed_forward.experts.gate_up_proj', 'model.layers.7.feed_forward.experts.down_proj', 'model.layers.7.feed_forward.experts.gate_up_proj', 'model.layers.8.feed_forward.experts.down_proj', 'model.layers.8.feed_forward.experts.gate_up_proj', 'model.layers.9.feed_forward.experts.down_proj', 'model.layers.9.feed_forward.experts.gate_up_proj']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "MODEL_ID = \"RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic\"\n",
    "\n",
    "device = \"auto\"\n",
    "# dtype = torch.bfloat16\n",
    "# dtype = torch.float8_e4m3fn\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=access_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    # torch_dtype=dtype,\n",
    "    device_map=device,\n",
    "    token=access_token,\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1eef88cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May  7 16:02:43 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.230.02             Driver Version: 535.230.02   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GH200 120GB             On  | 00000009:01:00.0 Off |                    0 |\n",
      "| N/A   27C    P0             137W / 900W |  88516MiB / 97871MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GH200 120GB             On  | 00000039:01:00.0 Off |                    0 |\n",
      "| N/A   29C    P0             138W / 900W |  92456MiB / 97871MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A    223406      C   ...mba/envs/data-extraction/bin/python    88508MiB |\n",
      "|    1   N/A  N/A    223406      C   ...mba/envs/data-extraction/bin/python    92438MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2db986d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.torch_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7bc8d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama4ForCausalLM(\n",
      "  (model): Llama4TextModel(\n",
      "    (embed_tokens): Embedding(202048, 5120, padding_idx=200018)\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x Llama4TextDecoderLayer(\n",
      "        (self_attn): Llama4TextAttention(\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (qk_norm): Llama4TextL2Norm(eps=1e-05)\n",
      "        )\n",
      "        (feed_forward): Llama4TextMoe(\n",
      "          (experts): Llama4TextExperts(\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (router): Linear(in_features=5120, out_features=16, bias=False)\n",
      "          (shared_expert): Llama4TextMLP(\n",
      "            (gate_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (up_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (down_proj): Linear(in_features=8192, out_features=5120, bias=False)\n",
      "            (activation_fn): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (input_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "        (post_attention_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "      )\n",
      "      (3): Llama4TextDecoderLayer(\n",
      "        (self_attn): Llama4TextAttention(\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "        )\n",
      "        (feed_forward): Llama4TextMoe(\n",
      "          (experts): Llama4TextExperts(\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (router): Linear(in_features=5120, out_features=16, bias=False)\n",
      "          (shared_expert): Llama4TextMLP(\n",
      "            (gate_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (up_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (down_proj): Linear(in_features=8192, out_features=5120, bias=False)\n",
      "            (activation_fn): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (input_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "        (post_attention_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "      )\n",
      "      (4-6): 3 x Llama4TextDecoderLayer(\n",
      "        (self_attn): Llama4TextAttention(\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (qk_norm): Llama4TextL2Norm(eps=1e-05)\n",
      "        )\n",
      "        (feed_forward): Llama4TextMoe(\n",
      "          (experts): Llama4TextExperts(\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (router): Linear(in_features=5120, out_features=16, bias=False)\n",
      "          (shared_expert): Llama4TextMLP(\n",
      "            (gate_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (up_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (down_proj): Linear(in_features=8192, out_features=5120, bias=False)\n",
      "            (activation_fn): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (input_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "        (post_attention_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "      )\n",
      "      (7): Llama4TextDecoderLayer(\n",
      "        (self_attn): Llama4TextAttention(\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "        )\n",
      "        (feed_forward): Llama4TextMoe(\n",
      "          (experts): Llama4TextExperts(\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (router): Linear(in_features=5120, out_features=16, bias=False)\n",
      "          (shared_expert): Llama4TextMLP(\n",
      "            (gate_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (up_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (down_proj): Linear(in_features=8192, out_features=5120, bias=False)\n",
      "            (activation_fn): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (input_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "        (post_attention_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "      )\n",
      "      (8-10): 3 x Llama4TextDecoderLayer(\n",
      "        (self_attn): Llama4TextAttention(\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (qk_norm): Llama4TextL2Norm(eps=1e-05)\n",
      "        )\n",
      "        (feed_forward): Llama4TextMoe(\n",
      "          (experts): Llama4TextExperts(\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (router): Linear(in_features=5120, out_features=16, bias=False)\n",
      "          (shared_expert): Llama4TextMLP(\n",
      "            (gate_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (up_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (down_proj): Linear(in_features=8192, out_features=5120, bias=False)\n",
      "            (activation_fn): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (input_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "        (post_attention_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "      )\n",
      "      (11): Llama4TextDecoderLayer(\n",
      "        (self_attn): Llama4TextAttention(\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "        )\n",
      "        (feed_forward): Llama4TextMoe(\n",
      "          (experts): Llama4TextExperts(\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (router): Linear(in_features=5120, out_features=16, bias=False)\n",
      "          (shared_expert): Llama4TextMLP(\n",
      "            (gate_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (up_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (down_proj): Linear(in_features=8192, out_features=5120, bias=False)\n",
      "            (activation_fn): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (input_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "        (post_attention_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "      )\n",
      "      (12-14): 3 x Llama4TextDecoderLayer(\n",
      "        (self_attn): Llama4TextAttention(\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (qk_norm): Llama4TextL2Norm(eps=1e-05)\n",
      "        )\n",
      "        (feed_forward): Llama4TextMoe(\n",
      "          (experts): Llama4TextExperts(\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (router): Linear(in_features=5120, out_features=16, bias=False)\n",
      "          (shared_expert): Llama4TextMLP(\n",
      "            (gate_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (up_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (down_proj): Linear(in_features=8192, out_features=5120, bias=False)\n",
      "            (activation_fn): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (input_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "        (post_attention_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "      )\n",
      "      (15): Llama4TextDecoderLayer(\n",
      "        (self_attn): Llama4TextAttention(\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "        )\n",
      "        (feed_forward): Llama4TextMoe(\n",
      "          (experts): Llama4TextExperts(\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (router): Linear(in_features=5120, out_features=16, bias=False)\n",
      "          (shared_expert): Llama4TextMLP(\n",
      "            (gate_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (up_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (down_proj): Linear(in_features=8192, out_features=5120, bias=False)\n",
      "            (activation_fn): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (input_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "        (post_attention_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "      )\n",
      "      (16-18): 3 x Llama4TextDecoderLayer(\n",
      "        (self_attn): Llama4TextAttention(\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (qk_norm): Llama4TextL2Norm(eps=1e-05)\n",
      "        )\n",
      "        (feed_forward): Llama4TextMoe(\n",
      "          (experts): Llama4TextExperts(\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (router): Linear(in_features=5120, out_features=16, bias=False)\n",
      "          (shared_expert): Llama4TextMLP(\n",
      "            (gate_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (up_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (down_proj): Linear(in_features=8192, out_features=5120, bias=False)\n",
      "            (activation_fn): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (input_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "        (post_attention_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "      )\n",
      "      (19): Llama4TextDecoderLayer(\n",
      "        (self_attn): Llama4TextAttention(\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "        )\n",
      "        (feed_forward): Llama4TextMoe(\n",
      "          (experts): Llama4TextExperts(\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (router): Linear(in_features=5120, out_features=16, bias=False)\n",
      "          (shared_expert): Llama4TextMLP(\n",
      "            (gate_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (up_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (down_proj): Linear(in_features=8192, out_features=5120, bias=False)\n",
      "            (activation_fn): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (input_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "        (post_attention_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "      )\n",
      "      (20-22): 3 x Llama4TextDecoderLayer(\n",
      "        (self_attn): Llama4TextAttention(\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (qk_norm): Llama4TextL2Norm(eps=1e-05)\n",
      "        )\n",
      "        (feed_forward): Llama4TextMoe(\n",
      "          (experts): Llama4TextExperts(\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (router): Linear(in_features=5120, out_features=16, bias=False)\n",
      "          (shared_expert): Llama4TextMLP(\n",
      "            (gate_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (up_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (down_proj): Linear(in_features=8192, out_features=5120, bias=False)\n",
      "            (activation_fn): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (input_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "        (post_attention_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "      )\n",
      "      (23): Llama4TextDecoderLayer(\n",
      "        (self_attn): Llama4TextAttention(\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "        )\n",
      "        (feed_forward): Llama4TextMoe(\n",
      "          (experts): Llama4TextExperts(\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (router): Linear(in_features=5120, out_features=16, bias=False)\n",
      "          (shared_expert): Llama4TextMLP(\n",
      "            (gate_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (up_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (down_proj): Linear(in_features=8192, out_features=5120, bias=False)\n",
      "            (activation_fn): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (input_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "        (post_attention_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "      )\n",
      "      (24-26): 3 x Llama4TextDecoderLayer(\n",
      "        (self_attn): Llama4TextAttention(\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (qk_norm): Llama4TextL2Norm(eps=1e-05)\n",
      "        )\n",
      "        (feed_forward): Llama4TextMoe(\n",
      "          (experts): Llama4TextExperts(\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (router): Linear(in_features=5120, out_features=16, bias=False)\n",
      "          (shared_expert): Llama4TextMLP(\n",
      "            (gate_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (up_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (down_proj): Linear(in_features=8192, out_features=5120, bias=False)\n",
      "            (activation_fn): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (input_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "        (post_attention_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "      )\n",
      "      (27): Llama4TextDecoderLayer(\n",
      "        (self_attn): Llama4TextAttention(\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "        )\n",
      "        (feed_forward): Llama4TextMoe(\n",
      "          (experts): Llama4TextExperts(\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (router): Linear(in_features=5120, out_features=16, bias=False)\n",
      "          (shared_expert): Llama4TextMLP(\n",
      "            (gate_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (up_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (down_proj): Linear(in_features=8192, out_features=5120, bias=False)\n",
      "            (activation_fn): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (input_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "        (post_attention_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "      )\n",
      "      (28-30): 3 x Llama4TextDecoderLayer(\n",
      "        (self_attn): Llama4TextAttention(\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (qk_norm): Llama4TextL2Norm(eps=1e-05)\n",
      "        )\n",
      "        (feed_forward): Llama4TextMoe(\n",
      "          (experts): Llama4TextExperts(\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (router): Linear(in_features=5120, out_features=16, bias=False)\n",
      "          (shared_expert): Llama4TextMLP(\n",
      "            (gate_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (up_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (down_proj): Linear(in_features=8192, out_features=5120, bias=False)\n",
      "            (activation_fn): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (input_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "        (post_attention_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "      )\n",
      "      (31): Llama4TextDecoderLayer(\n",
      "        (self_attn): Llama4TextAttention(\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "        )\n",
      "        (feed_forward): Llama4TextMoe(\n",
      "          (experts): Llama4TextExperts(\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (router): Linear(in_features=5120, out_features=16, bias=False)\n",
      "          (shared_expert): Llama4TextMLP(\n",
      "            (gate_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (up_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (down_proj): Linear(in_features=8192, out_features=5120, bias=False)\n",
      "            (activation_fn): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (input_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "        (post_attention_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "      )\n",
      "      (32-34): 3 x Llama4TextDecoderLayer(\n",
      "        (self_attn): Llama4TextAttention(\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (qk_norm): Llama4TextL2Norm(eps=1e-05)\n",
      "        )\n",
      "        (feed_forward): Llama4TextMoe(\n",
      "          (experts): Llama4TextExperts(\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (router): Linear(in_features=5120, out_features=16, bias=False)\n",
      "          (shared_expert): Llama4TextMLP(\n",
      "            (gate_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (up_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (down_proj): Linear(in_features=8192, out_features=5120, bias=False)\n",
      "            (activation_fn): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (input_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "        (post_attention_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "      )\n",
      "      (35): Llama4TextDecoderLayer(\n",
      "        (self_attn): Llama4TextAttention(\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "        )\n",
      "        (feed_forward): Llama4TextMoe(\n",
      "          (experts): Llama4TextExperts(\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (router): Linear(in_features=5120, out_features=16, bias=False)\n",
      "          (shared_expert): Llama4TextMLP(\n",
      "            (gate_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (up_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (down_proj): Linear(in_features=8192, out_features=5120, bias=False)\n",
      "            (activation_fn): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (input_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "        (post_attention_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "      )\n",
      "      (36-38): 3 x Llama4TextDecoderLayer(\n",
      "        (self_attn): Llama4TextAttention(\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (qk_norm): Llama4TextL2Norm(eps=1e-05)\n",
      "        )\n",
      "        (feed_forward): Llama4TextMoe(\n",
      "          (experts): Llama4TextExperts(\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (router): Linear(in_features=5120, out_features=16, bias=False)\n",
      "          (shared_expert): Llama4TextMLP(\n",
      "            (gate_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (up_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (down_proj): Linear(in_features=8192, out_features=5120, bias=False)\n",
      "            (activation_fn): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (input_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "        (post_attention_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "      )\n",
      "      (39): Llama4TextDecoderLayer(\n",
      "        (self_attn): Llama4TextAttention(\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "        )\n",
      "        (feed_forward): Llama4TextMoe(\n",
      "          (experts): Llama4TextExperts(\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (router): Linear(in_features=5120, out_features=16, bias=False)\n",
      "          (shared_expert): Llama4TextMLP(\n",
      "            (gate_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (up_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (down_proj): Linear(in_features=8192, out_features=5120, bias=False)\n",
      "            (activation_fn): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (input_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "        (post_attention_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "      )\n",
      "      (40-42): 3 x Llama4TextDecoderLayer(\n",
      "        (self_attn): Llama4TextAttention(\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (qk_norm): Llama4TextL2Norm(eps=1e-05)\n",
      "        )\n",
      "        (feed_forward): Llama4TextMoe(\n",
      "          (experts): Llama4TextExperts(\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (router): Linear(in_features=5120, out_features=16, bias=False)\n",
      "          (shared_expert): Llama4TextMLP(\n",
      "            (gate_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (up_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (down_proj): Linear(in_features=8192, out_features=5120, bias=False)\n",
      "            (activation_fn): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (input_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "        (post_attention_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "      )\n",
      "      (43): Llama4TextDecoderLayer(\n",
      "        (self_attn): Llama4TextAttention(\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "        )\n",
      "        (feed_forward): Llama4TextMoe(\n",
      "          (experts): Llama4TextExperts(\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (router): Linear(in_features=5120, out_features=16, bias=False)\n",
      "          (shared_expert): Llama4TextMLP(\n",
      "            (gate_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (up_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (down_proj): Linear(in_features=8192, out_features=5120, bias=False)\n",
      "            (activation_fn): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (input_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "        (post_attention_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "      )\n",
      "      (44-46): 3 x Llama4TextDecoderLayer(\n",
      "        (self_attn): Llama4TextAttention(\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (qk_norm): Llama4TextL2Norm(eps=1e-05)\n",
      "        )\n",
      "        (feed_forward): Llama4TextMoe(\n",
      "          (experts): Llama4TextExperts(\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (router): Linear(in_features=5120, out_features=16, bias=False)\n",
      "          (shared_expert): Llama4TextMLP(\n",
      "            (gate_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (up_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (down_proj): Linear(in_features=8192, out_features=5120, bias=False)\n",
      "            (activation_fn): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (input_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "        (post_attention_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "      )\n",
      "      (47): Llama4TextDecoderLayer(\n",
      "        (self_attn): Llama4TextAttention(\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "        )\n",
      "        (feed_forward): Llama4TextMoe(\n",
      "          (experts): Llama4TextExperts(\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (router): Linear(in_features=5120, out_features=16, bias=False)\n",
      "          (shared_expert): Llama4TextMLP(\n",
      "            (gate_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (up_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
      "            (down_proj): Linear(in_features=8192, out_features=5120, bias=False)\n",
      "            (activation_fn): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (input_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "        (post_attention_layernorm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): Llama4TextRMSNorm((5120,), eps=1e-05)\n",
      "    (rotary_emb): Llama4TextRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=5120, out_features=202048, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b69036e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1034])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[200000, 200005,  15651,  ..., 140680, 200006,    368]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = message_metadata\n",
    "# messages = message_results\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    conversation=messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "print(input_ids.shape)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76ae1242",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/user/1483800663/ipykernel_223406/3990874808.py:5: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.26 GiB. GPU 0 has a total capacity of 95.00 GiB of which 3.64 GiB is free. Including non-PyTorch memory, this process has 91.35 GiB memory in use. Of the allocated memory 89.58 GiB is allocated by PyTorch, and 1.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m terminators = [\n\u001b[32m      2\u001b[39m     tokenizer.eos_token_id,\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\u001b[39;00m\n\u001b[32m      4\u001b[39m ]\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.cuda.amp.autocast():\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mterminators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# top_p=0.95,\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(outputs.shape)\n\u001b[32m     15\u001b[39m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/data-extraction/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/data-extraction/lib/python3.12/site-packages/transformers/generation/utils.py:2465\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[39m\n\u001b[32m   2457\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2458\u001b[39m         input_ids=input_ids,\n\u001b[32m   2459\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2460\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2461\u001b[39m         **model_kwargs,\n\u001b[32m   2462\u001b[39m     )\n\u001b[32m   2464\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2465\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2466\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2467\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2468\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2469\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2470\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2471\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2472\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2473\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2475\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2476\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2477\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2478\u001b[39m         input_ids=input_ids,\n\u001b[32m   2479\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2480\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2481\u001b[39m         **model_kwargs,\n\u001b[32m   2482\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/data-extraction/lib/python3.12/site-packages/transformers/generation/utils.py:3431\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3428\u001b[39m model_inputs.update({\u001b[33m\"\u001b[39m\u001b[33moutput_hidden_states\u001b[39m\u001b[33m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[32m   3430\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[32m-> \u001b[39m\u001b[32m3431\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3432\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3433\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/data-extraction/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/data-extraction/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/data-extraction/lib/python3.12/site-packages/accelerate/hooks.py:176\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/data-extraction/lib/python3.12/site-packages/transformers/models/llama4/modeling_llama4.py:1030\u001b[39m, in \u001b[36mLlama4ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m   1027\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m   1029\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1030\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1033\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1036\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1037\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1038\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1039\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1040\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1041\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1044\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1045\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/data-extraction/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/data-extraction/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/data-extraction/lib/python3.12/site-packages/transformers/models/llama4/modeling_llama4.py:701\u001b[39m, in \u001b[36mLlama4TextModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    687\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    688\u001b[39m         decoder_layer.\u001b[34m__call__\u001b[39m,\n\u001b[32m    689\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    698\u001b[39m         freq_cis,\n\u001b[32m    699\u001b[39m     )\n\u001b[32m    700\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m     layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk_causal_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_causal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    706\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfreq_cis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    714\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/data-extraction/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/data-extraction/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/data-extraction/lib/python3.12/site-packages/accelerate/hooks.py:176\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/data-extraction/lib/python3.12/site-packages/transformers/models/llama4/modeling_llama4.py:435\u001b[39m, in \u001b[36mLlama4TextDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, chunk_causal_mask, position_ids, past_key_value, output_attentions, output_router_logits, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    432\u001b[39m     attention_mask = chunk_causal_mask\n\u001b[32m    434\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m attention_states, self_attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m hidden_states = residual + attention_states\n\u001b[32m    447\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/data-extraction/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/data-extraction/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/data-extraction/lib/python3.12/site-packages/accelerate/hooks.py:176\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/data-extraction/lib/python3.12/site-packages/transformers/models/llama4/modeling_llama4.py:379\u001b[39m, in \u001b[36mLlama4TextAttention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[39m\n\u001b[32m    377\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    378\u001b[39m         attention_interface = ALL_ATTENTION_FUNCTIONS[\u001b[38;5;28mself\u001b[39m.config._attn_implementation]\n\u001b[32m--> \u001b[39m\u001b[32m379\u001b[39m attn_output, attn_weights = \u001b[43mattention_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    390\u001b[39m attn_output = attn_output.reshape(*input_shape, -\u001b[32m1\u001b[39m).contiguous()\n\u001b[32m    391\u001b[39m attn_output = \u001b[38;5;28mself\u001b[39m.o_proj(attn_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/data-extraction/lib/python3.12/site-packages/transformers/models/llama4/modeling_llama4.py:286\u001b[39m, in \u001b[36meager_attention_forward\u001b[39m\u001b[34m(module, query, key, value, attention_mask, scaling, dropout, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    285\u001b[39m     causal_mask = attention_mask[:, :, :, : key_states.shape[-\u001b[32m2\u001b[39m]]\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     attn_weights = \u001b[43mattn_weights\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mcausal_mask\u001b[49m\n\u001b[32m    288\u001b[39m attn_weights = nn.functional.softmax(attn_weights.float(), dim=-\u001b[32m1\u001b[39m).to(query.dtype)\n\u001b[32m    289\u001b[39m attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 1.26 GiB. GPU 0 has a total capacity of 95.00 GiB of which 3.64 GiB is free. Including non-PyTorch memory, this process has 91.35 GiB memory in use. Of the allocated memory 89.58 GiB is allocated by PyTorch, and 1.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    # tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "]\n",
    "with torch.cuda.amp.autocast():\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=2048,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.1,\n",
    "        # top_p=0.95,\n",
    "    )\n",
    "print(outputs.shape)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ede6927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a data scientist responsible for extracting accurate information from research papers. You answer each question with a single JSON string.<｜User｜>\\n                This is an abstract from a Mendelian randomization study.\\n                    \"Alcohol consumption significantly impacts disease burden and has been linked to various diseases in observational studies. However, comprehensive meta-analyses using Mendelian randomization (MR) to examine drinking patterns are limited. We aimed to evaluate the health risks of alcohol use by integrating findings from MR studies. A thorough search was conducted for MR studies focused on alcohol exposure. We utilized two sets of instrumental variables-alcohol consumption and problematic alcohol use-and summary statistics from the FinnGen consortium R9 release to perform de novo MR analyses. Our meta-analysis encompassed 64 published and 151 de novo MR analyses across 76 distinct primary outcomes. Results show that a genetic predisposition to alcohol consumption, independent of smoking, significantly correlates with a decreased risk of Parkinson\\'s disease, prostate hyperplasia, and rheumatoid arthritis. It was also associated with an increased risk of chronic pancreatitis, colorectal cancer, and head and neck cancers. Additionally, a genetic predisposition to problematic alcohol use is strongly associated with increased risks of alcoholic liver disease, cirrhosis, both acute and chronic pancreatitis, and pneumonia. Evidence from our MR study supports the notion that alcohol consumption and problematic alcohol use are causally associated with a range of diseases, predominantly by increasing the risk.\"   \\n                    <｜Assistant｜>This is an example output in JSON format: \\n    { \"metadata\": {\\n    \"exposures\": [\\n    {\\n        \"id\": \"1\",\\n        \"trait\": \"Particulate matter 2.5\",\\n        \"category\": \"Environmental\"\\n    },\\n    {\\n        \"id\": \"2\",\\n        \"trait\": \"Type 2 diabetes\",\\n        \"category\": \"metabolic disease\"\\n    },\\n    {\\n        \"id\": \"3\",\\n        \"trait\": \"Body mass index\",\\n        \"category\": \"Anthropometric\"\\n    }\\n    ],\\n    \"outcomes\": [\\n    {\\n        \"id\": \"1\",\\n        \"trait\": \"Forced expiratory volume in 1 s\",\\n        \"category\": \"Clinical measure\"\\n    },\\n    {\\n        \"id\": \"2\",\\n        \"trait\": \"Forced vital capacity\",\\n        \"category\": \"Clinical measure\"\\n    },\\n    {\\n        \"id\": \"3\",\\n        \"trait\": \"Gastroesophageal reflux disease\",\\n        \"category\": \"disease of the digestive system\"\\n    },\\n    {\\n        \"id\": \"4\",\\n        \"trait\": \"Non-alcoholic fatty liver disease (NAFLD)\",\\n        \"category\": \"disease of the digestive system\"\\n    }\\n    ],\\n    \"methods\": [\"two-sample mendelian randomization\", \"multivariable mendelian randomization\", \"colocalisation\", \"network mendelian randomization\"],\\n    \"population\": [\"European men\", \"Breast cancer patients\", \"African-Americans\"],\\n    \"metainformation\": {\\n        \"error\": \"No information on population is provided in abstract\",\\n        \"explanation\": \"Some methods do not match those listed in the prompt\"\\n    }\\n    }\\n    }\\n    <｜User｜>What are the exposures, outcomes in this abstract? If there are multiple exposures or outcomes, provide them all. If there are no exposures or outcomes, provide an empty list. Also categorize the exposures and outcomes into the following groups using the exact category names provided: \\n- molecular\\n- socioeconomic\\n- environmental\\n- behavioural\\n- anthropometric\\n- clinical measures\\n- infectious disease\\n- neoplasm\\n- disease of the blood and blood-forming organs\\n- metabolic disease\\n- mental disorder\\n- disease of the nervous system\\n- disease of the eye and adnexa\\n- disease of the ear and mastoid process\\n- disease of the circulatory system\\n- disease of the digestive system\\n- disease of the skin and subcutaneous tissue\\n- disease of the musculoskeletal system and connective tissue\\n- disease of the genitourinary system\\nIf an exposure or outcome does not fit into any of these groups, specify \"Other\". \\n\\nList the analytical methods used in the abstract. Match the methods to the following list of exact method names. If a method is used that is not in the list, specify \"Other\" and also provide the name of the method. The list of methods is as follows:\\n- two-sample mendelian randomization\\n- multivariable mendelian randomization\\n- colocalization\\n- network mendelian randomization\\n- triangulation\\n- reverse mendelian randomization\\n- one-sample mendelian randomization\\n- negative controls\\n- sensitivity analysis\\n- non-linear mendelian randomization\\n- within-family mendelian randomization\\n\\nProvide a description of the population(s) on which the study described in the abstract was based.\\n\\nProvide your answer in strict pretty JSON format using exactly the format as the example output and without markdown code blocks. Any error messages and explanations must be included in the JSON output with the key \"metainformation\".\\n<｜Assistant｜><think>\\nAlright, so I need to figure out the exposures and outcomes from the given abstract. The abstract talks about alcohol consumption and problematic alcohol use as exposures. For the outcomes, it mentions several diseases like Parkinson\\'s disease, prostate hyperplasia, rheumatoid arthritis, chronic pancreatitis, colorectal cancer, head and neck cancers, alcoholic liver disease, cirrhosis, pneumonia, and more.\\n\\nNext, I have to categorize these. Alcohol consumption and problematic alcohol use are both behavioral, so they fall under the \\'behavioural\\' category. The outcomes are various diseases, so I\\'ll categorize each under the appropriate disease group. For example, Parkinson\\'s disease is a nervous system disease, prostate hyperplasia is a neoplasm, and so on.\\n\\nNow, looking at the methods used in the abstract, it mentions Mendelian randomization (MR) with two sets of instrumental variables and a meta-analysis. The methods listed in the example include things like two-sample MR, multivariable MR, colocalization, etc. The abstract uses de novo MR analyses and a meta-analysis approach, which aren\\'t exactly the same as the listed methods. So, I\\'ll mark these as \"Other\" with the specific methods mentioned.\\n\\nThe population is described as being based on the FinnGen consortium R9 release, which includes European men and breast cancer patients. However, the abstract doesn\\'t specify the exact population details, so I\\'ll note that as an error.\\n\\nI need to make sure the JSON structure matches the example exactly, with all the required keys like metadata, methods, population, and metainformation. Each exposure and outcome should be listed with their respective IDs, traits, and categories. Any discrepancies or missing information should be clearly indicated in the metainformation section.\\n\\nI should also ensure that the JSON is properly formatted with pretty printing and correct syntax to avoid any errors. Each part needs to be double-checked to make sure nothing is missed or incorrectly categorized.\\n\\nFinally, I\\'ll compile all this information into the JSON structure, making sure it\\'s accurate and follows the specified format.\\n</think>\\n\\n```json\\n{\\n    \"metadata\": {\\n        \"exposures\": [\\n            {\\n                \"id\": \"1\",\\n                \"trait\": \"Alcohol consumption\",\\n                \"category\": \"behavioural\"\\n            },\\n            {\\n                \"id\": \"2\",\\n                \"trait\": \"Problematic alcohol use\",\\n                \"category\": \"behavioural\"\\n            }\\n        ],\\n        \"outcomes\": [\\n            {\\n                \"id\": \"1\",\\n                \"trait\": \"Parkinson\\'s disease\",\\n                \"category\": \"disease of the nervous system\"\\n            },\\n            {\\n                \"id\": \"2\",\\n                \"trait\": \"Prostate hyperplasia\",\\n                \"category\": \"neoplasm\"\\n            },\\n            {\\n                \"id\": \"3\",\\n                \"trait\": \"Rheumatoid arthritis\",\\n                \"category\": \"metabolic disease\"\\n            },\\n            {\\n                \"id\": \"4\",\\n                \"trait\": \"Chronic pancreatitis\",\\n                \"category\": \"disease of the digestive system\"\\n            },\\n            {\\n                \"id\": \"5\",\\n                \"trait\": \"Colorectal cancer\",\\n                \"category\": \"neoplasm\"\\n            },\\n            {\\n                \"id\": \"6\",\\n                \"trait\": \"Head and neck cancers\",\\n                \"category\": \"neoplasm\"\\n            },\\n            {\\n                \"id\": \"7\",\\n                \"trait\": \"Alcoholic liver disease\",\\n                \"category\": \"disease of the digestive system\"\\n            },\\n            {\\n                \"id\": \"8\",\\n                \"trait\": \"Cirrhosis\",\\n                \"category\": \"disease of the digestive system\"\\n            },\\n            {\\n                \"id\": \"9\",\\n                \"trait\": \"Pneumonia\",\\n                \"category\": \"disease of the circulatory system\"\\n            }\\n        ]\\n    },\\n    \"methods\": [\\n        \"de novo Mendelian randomization\",\\n        \"meta-analysis\"\\n    ],\\n    \"population\": [\\n        \"FinnGen consortium R9 release\"\\n    ],\\n    \"metainformation\": {\\n        \"error\": \"Population details not specified in the abstract\",\\n        \"explanation\": \"The study population is inferred from the FinnGen consortium R9 release but specific details are not provided.\"\\n    }\\n}\\n```'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a3b2d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alright, so I need to figure out the exposures and outcomes from the given abstract. The abstract talks about alcohol consumption and problematic alcohol use as exposures. For the outcomes, it mentions several diseases like Parkinson's disease, prostate hyperplasia, rheumatoid arthritis, chronic pancreatitis, colorectal cancer, head and neck cancers, alcoholic liver disease, cirrhosis, pneumonia, and more.\n",
      "\n",
      "Next, I have to categorize these. Alcohol consumption and problematic alcohol use are both behavioral, so they fall under the 'behavioural' category. The outcomes are various diseases, so I'll categorize each under the appropriate disease group. For example, Parkinson's disease is a nervous system disease, prostate hyperplasia is a neoplasm, and so on.\n",
      "\n",
      "Now, looking at the methods used in the abstract, it mentions Mendelian randomization (MR) with two sets of instrumental variables and a meta-analysis. The methods listed in the example include things like two-sample MR, multivariable MR, colocalization, etc. The abstract uses de novo MR analyses and a meta-analysis approach, which aren't exactly the same as the listed methods. So, I'll mark these as \"Other\" with the specific methods mentioned.\n",
      "\n",
      "The population is described as being based on the FinnGen consortium R9 release, which includes European men and breast cancer patients. However, the abstract doesn't specify the exact population details, so I'll note that as an error.\n",
      "\n",
      "I need to make sure the JSON structure matches the example exactly, with all the required keys like metadata, methods, population, and metainformation. Each exposure and outcome should be listed with their respective IDs, traits, and categories. Any discrepancies or missing information should be clearly indicated in the metainformation section.\n",
      "\n",
      "I should also ensure that the JSON is properly formatted with pretty printing and correct syntax to avoid any errors. Each part needs to be double-checked to make sure nothing is missed or incorrectly categorized.\n",
      "\n",
      "Finally, I'll compile all this information into the JSON structure, making sure it's accurate and follows the specified format.\n",
      "</think>\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"metadata\": {\n",
      "        \"exposures\": [\n",
      "            {\n",
      "                \"id\": \"1\",\n",
      "                \"trait\": \"Alcohol consumption\",\n",
      "                \"category\": \"behavioural\"\n",
      "            },\n",
      "            {\n",
      "                \"id\": \"2\",\n",
      "                \"trait\": \"Problematic alcohol use\",\n",
      "                \"category\": \"behavioural\"\n",
      "            }\n",
      "        ],\n",
      "        \"outcomes\": [\n",
      "            {\n",
      "                \"id\": \"1\",\n",
      "                \"trait\": \"Parkinson's disease\",\n",
      "                \"category\": \"disease of the nervous system\"\n",
      "            },\n",
      "            {\n",
      "                \"id\": \"2\",\n",
      "                \"trait\": \"Prostate hyperplasia\",\n",
      "                \"category\": \"neoplasm\"\n",
      "            },\n",
      "            {\n",
      "                \"id\": \"3\",\n",
      "                \"trait\": \"Rheumatoid arthritis\",\n",
      "                \"category\": \"metabolic disease\"\n",
      "            },\n",
      "            {\n",
      "                \"id\": \"4\",\n",
      "                \"trait\": \"Chronic pancreatitis\",\n",
      "                \"category\": \"disease of the digestive system\"\n",
      "            },\n",
      "            {\n",
      "                \"id\": \"5\",\n",
      "                \"trait\": \"Colorectal cancer\",\n",
      "                \"category\": \"neoplasm\"\n",
      "            },\n",
      "            {\n",
      "                \"id\": \"6\",\n",
      "                \"trait\": \"Head and neck cancers\",\n",
      "                \"category\": \"neoplasm\"\n",
      "            },\n",
      "            {\n",
      "                \"id\": \"7\",\n",
      "                \"trait\": \"Alcoholic liver disease\",\n",
      "                \"category\": \"disease of the digestive system\"\n",
      "            },\n",
      "            {\n",
      "                \"id\": \"8\",\n",
      "                \"trait\": \"Cirrhosis\",\n",
      "                \"category\": \"disease of the digestive system\"\n",
      "            },\n",
      "            {\n",
      "                \"id\": \"9\",\n",
      "                \"trait\": \"Pneumonia\",\n",
      "                \"category\": \"disease of the circulatory system\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"methods\": [\n",
      "        \"de novo Mendelian randomization\",\n",
      "        \"meta-analysis\"\n",
      "    ],\n",
      "    \"population\": [\n",
      "        \"FinnGen consortium R9 release\"\n",
      "    ],\n",
      "    \"metainformation\": {\n",
      "        \"error\": \"Population details not specified in the abstract\",\n",
      "        \"explanation\": \"The study population is inferred from the FinnGen consortium R9 release but specific details are not provided.\"\n",
      "    }\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = outputs[0][input_ids.shape[-1] :]\n",
    "result = tokenizer.decode(response, skip_special_tokens=True)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
