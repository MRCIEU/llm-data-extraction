{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "743c40a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May  6 20:51:53 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.230.02             Driver Version: 535.230.02   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GH200 120GB             On  | 00000009:01:00.0 Off |                    0 |\n",
      "| N/A   26C    P0              92W / 900W |   5413MiB / 97871MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GH200 120GB             On  | 00000039:01:00.0 Off |                    0 |\n",
      "| N/A   27C    P0              89W / 900W |   5426MiB / 97871MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0a85cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b5a/ik18445.b5a/micromamba/envs/data-extraction/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.51.3\n",
      "2.6.0\n",
      "True\n",
      "12.6\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from environs import env\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "from local_funcs import prompt_funcs\n",
    "from yiutils.project_utils import find_project_root\n",
    "\n",
    "proj_root = find_project_root(\"justfile\")\n",
    "data_dir = proj_root / \"data\"\n",
    "\n",
    "print(transformers.__version__)\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "\n",
    "env.read_env(proj_root / \".env\")\n",
    "access_token = env(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "path_to_mr_pubmed_data = (\n",
    "    data_dir / \"intermediate\" / \"mr-pubmed-data\" / \"mr-pubmed-data.json\"\n",
    ")\n",
    "assert path_to_mr_pubmed_data.exists(), (\n",
    "    f\"Data file {path_to_mr_pubmed_data} does not exist.\"\n",
    ")\n",
    "\n",
    "with open(path_to_mr_pubmed_data, \"r\") as f:\n",
    "    mr_pubmed_data = json.load(f)\n",
    "\n",
    "article_data = mr_pubmed_data[0]\n",
    "\n",
    "message_metadata = prompt_funcs.make_message_metadata(article_data[\"ab\"])\n",
    "message_results = prompt_funcs.make_message_results(article_data[\"ab\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09593b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 9/9 [00:37<00:00,  4.19s/it]\n"
     ]
    }
   ],
   "source": [
    "MODEL_ID = \"tiiuae/falcon-40b-instruct\"\n",
    "\n",
    "device = \"auto\"\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=access_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=device,\n",
    "    token=access_token,\n",
    "    # trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b69036e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m messages = message_metadata\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# messages = message_results\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m input_ids = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconversation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m.to(model.device)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(input_ids.shape)\n\u001b[32m      7\u001b[39m input_ids\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/data-extraction/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1629\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.apply_chat_template\u001b[39m\u001b[34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[39m\n\u001b[32m   1626\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1627\u001b[39m     tokenizer_kwargs = {}\n\u001b[32m-> \u001b[39m\u001b[32m1629\u001b[39m chat_template = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1631\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_assistant_tokens_mask \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re.search(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m{\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m%\u001b[39m\u001b[33m-?\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms*generation\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms*-?\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m%\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m}\u001b[39m\u001b[33m\"\u001b[39m, chat_template):\n\u001b[32m   1632\u001b[39m     logger.warning_once(\n\u001b[32m   1633\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mreturn_assistant_tokens_mask==True but chat template does not contain `\u001b[39m\u001b[33m{\u001b[39m\u001b[38;5;132;01m% g\u001b[39;00m\u001b[33meneration \u001b[39m\u001b[33m%\u001b[39m\u001b[33m}` keyword.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1634\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/data-extraction/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1822\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.get_chat_template\u001b[39m\u001b[34m(self, chat_template, tools)\u001b[39m\n\u001b[32m   1820\u001b[39m         chat_template = \u001b[38;5;28mself\u001b[39m.chat_template\n\u001b[32m   1821\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1823\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCannot use chat template functions because tokenizer.chat_template is not set and no template \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1824\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33margument was passed! For information about writing templates and setting the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1825\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtokenizer.chat_template attribute, please see the documentation at \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1826\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mhttps://huggingface.co/docs/transformers/main/en/chat_templating\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1827\u001b[39m         )\n\u001b[32m   1829\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m chat_template\n",
      "\u001b[31mValueError\u001b[39m: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating"
     ]
    }
   ],
   "source": [
    "messages = message_metadata\n",
    "# messages = message_results\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    conversation=messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "print(input_ids.shape)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76ae1242",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1561])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1357,  362,  241,  ..., 1881,   25,   11]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    # tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "]\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=2048,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    # top_p=0.95,\n",
    ")\n",
    "print(outputs.shape)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ede6927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a data scientist responsible for extracting accurate information from research papers. You answer each question with a single JSON string.\\n\\nUser: This is an abstract from a Mendelian randomization study.\\n                    \"Alcohol consumption significantly impacts disease burden and has been linked to various diseases in observational studies. However, comprehensive meta-analyses using Mendelian randomization (MR) to examine drinking patterns are limited. We aimed to evaluate the health risks of alcohol use by integrating findings from MR studies. A thorough search was conducted for MR studies focused on alcohol exposure. We utilized two sets of instrumental variables-alcohol consumption and problematic alcohol use-and summary statistics from the FinnGen consortium R9 release to perform de novo MR analyses. Our meta-analysis encompassed 64 published and 151 de novo MR analyses across 76 distinct primary outcomes. Results show that a genetic predisposition to alcohol consumption, independent of smoking, significantly correlates with a decreased risk of Parkinson\\'s disease, prostate hyperplasia, and rheumatoid arthritis. It was also associated with an increased risk of chronic pancreatitis, colorectal cancer, and head and neck cancers. Additionally, a genetic predisposition to problematic alcohol use is strongly associated with increased risks of alcoholic liver disease, cirrhosis, both acute and chronic pancreatitis, and pneumonia. Evidence from our MR study supports the notion that alcohol consumption and problematic alcohol use are causally associated with a range of diseases, predominantly by increasing the risk.\"\\n\\nAssistant: This is an example output in JSON format: \\n    {\\n    \"results\": [\\n        {\\n            \"exposure\": \"Particulate matter 2.5\"},\\n            \"outcome\": \"Forced expiratory volume in 1 s\"\\n            \"beta\": 0.154,\\n            \"units\": \"mmHg\",\\n            \"hazard ratio\": null,\\n            \"odds ratio\": null,\\n            \"95% CI\": [0.101,0.215],\\n            \"SE\": 0.102,\\n            \"P-value\": 0.0015,\\n            \"Direction\": \"increases\"\\n        },\\n        {\\n            \"exposure\": \"Body mass index\"},\\n            \"outcome\": \"Gastroesophageal reflux disease\"\\n            \"beta\": null,\\n            \"units\": null,\\n            \"hazard ratio\": null,\\n            \"odds ratio\": 1.114,\\n            \"95% CI\": [1.021,1.314],\\n            \"SE\": null,\\n            \"P-value\": 0.0157,\\n            \"Direction\": \"increases\"\\n        },\\n        {\\n            \"exposure\": \"Body mass index\"},\\n            \"outcome\": \"Non-alcoholic fatty liver disease (NAFLD)\"\\n            \"beta\": null,\\n            \"units\": null,\\n            \"hazard ratio\": null,\\n            \"odds ratio\": null,\\n            \"95% CI\": [null,null],\\n            \"SE\": null,\\n            \"P-value\": null,\\n            \"Direction\": \"increases\"\\n        }\\n    ]\\n    \"resultsinformation\": {\\n        \"error\": \"No results provided in abstract\",\\n        \"explanation\": \"P-values were string, not numeric values\"\\n    }\\n    }\\n\\nUser: List all of the results in the abstract, with each entry comprising: exposure, outcome, beta, units, odds ratio, hazard ratio, 95% confidence interval, standard error, and P-value. If any of these fields is missing, substitute them with \"null\". Add a field called \"direction\" which describes whether the exposure \"increases\" or \"decreases\" the outcome. \\nProvide your answer in strict pretty JSON format using exactly the format as the example output and without markdown code blocks. You must only include values explicitly written in the abstract. Any error messages and explanations must be included in the JSON output with the key \"resultsinformation\".\\n\\nAssistant: This is an example output in JSON format: \\n    {\\n    \"results\": [\\n        {\\n            \"exposure\": \"Particulate matter 2.5\"},\\n            \"outcome\": \"Forced expiratory volume in 1 s\"\\n            \"beta\": 0.154,\\n            \"units\": \"mmHg\",\\n            \"hazard ratio\": null,\\n            \"odds ratio\": null,\\n            \"95% CI\": [0.101,0.215],\\n            \"SE\": 0.102,\\n            \"P-value\": 0.0015,\\n            \"direction\": \"increases\"\\n        },\\n        {\\n            \"exposure\": \"Body mass index\"},\\n            \"outcome\": \"Gastroesophageal reflux disease\"\\n            \"beta\": null,\\n            \"units\": null,\\n            \"hazard ratio\": null,\\n            \"odds ratio\": 1.114,\\n            \"95% CI\": [1.021,1.314],\\n            \"SE\": null,\\n            \"P-value\": 0.0157,\\n            \"direction\": \"increases\"\\n        },\\n        {\\n            \"exposure\": \"Body mass index\"},\\n            \"outcome\": \"Non-alcoholic fatty liver disease (NAFLD)\"\\n            \"beta\": null,\\n            \"units\": null,\\n            \"hazard ratio\": null,\\n            \"odds ratio\": null,\\n            \"95% CI\": [null,null],\\n            \"SE\": null,\\n            \"P-value\": null,\\n            \"direction\": \"increases\"\\n        }\\n    ]\\n    \"resultsinformation\": {\\n        \"error\": \"No results provided in abstract\",\\n        \"explanation\": \"P-values were string, not numeric values\"\\n    }\\n    }\\n\\nThe output should be in strict pretty JSON format, with the following fields:\\n\\n- \"results\": an array of objects, each containing a single result.\\n- \"results.exposure\": the exposure variable.\\n- \"results.outcome\": the outcome variable.\\n- \"results.beta\": the beta variable.\\n- \"results.units\": the units variable.\\n- \"results.hazard_ratio\": the hazard ratio variable.\\n- \"results.odds_ratio\": the odds ratio variable.\\n- \"results.direction\": the direction variable.\\n- \"results.SE\": the standard error of the estimate.\\n- \"results.P-value\": the P-value for the estimate.\\n- \"results.error\": an error message if the results are missing.\\n- \"results.explanation\": an explanation of the results.\\n\\nThe \"results.beta\" variable should be a numeric value, not a string. The \"results.units\" variable should be a numeric value, not a string. The \"results.hazard_ratio\" variable should be a numeric value, not a string. The \"results.odds_ratio\" variable should be a numeric value, not a string. The \"results.direction\" variable should be a string, not a numeric value. The \"results.SE\" variable should be a numeric value, not a string. The \"results.P-value\" variable should be a numeric value, not a string. The \"results.error\" variable should be a string, not a numeric value. The \"results.explanation\" variable should be a string, not a numeric value.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a3b2d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This is an example output in JSON format: \n",
      "    {\n",
      "    \"results\": [\n",
      "        {\n",
      "            \"exposure\": \"Particulate matter 2.5\"},\n",
      "            \"outcome\": \"Forced expiratory volume in 1 s\"\n",
      "            \"beta\": 0.154,\n",
      "            \"units\": \"mmHg\",\n",
      "            \"hazard ratio\": null,\n",
      "            \"odds ratio\": null,\n",
      "            \"95% CI\": [0.101,0.215],\n",
      "            \"SE\": 0.102,\n",
      "            \"P-value\": 0.0015,\n",
      "            \"direction\": \"increases\"\n",
      "        },\n",
      "        {\n",
      "            \"exposure\": \"Body mass index\"},\n",
      "            \"outcome\": \"Gastroesophageal reflux disease\"\n",
      "            \"beta\": null,\n",
      "            \"units\": null,\n",
      "            \"hazard ratio\": null,\n",
      "            \"odds ratio\": 1.114,\n",
      "            \"95% CI\": [1.021,1.314],\n",
      "            \"SE\": null,\n",
      "            \"P-value\": 0.0157,\n",
      "            \"direction\": \"increases\"\n",
      "        },\n",
      "        {\n",
      "            \"exposure\": \"Body mass index\"},\n",
      "            \"outcome\": \"Non-alcoholic fatty liver disease (NAFLD)\"\n",
      "            \"beta\": null,\n",
      "            \"units\": null,\n",
      "            \"hazard ratio\": null,\n",
      "            \"odds ratio\": null,\n",
      "            \"95% CI\": [null,null],\n",
      "            \"SE\": null,\n",
      "            \"P-value\": null,\n",
      "            \"direction\": \"increases\"\n",
      "        }\n",
      "    ]\n",
      "    \"resultsinformation\": {\n",
      "        \"error\": \"No results provided in abstract\",\n",
      "        \"explanation\": \"P-values were string, not numeric values\"\n",
      "    }\n",
      "    }\n",
      "\n",
      "The output should be in strict pretty JSON format, with the following fields:\n",
      "\n",
      "- \"results\": an array of objects, each containing a single result.\n",
      "- \"results.exposure\": the exposure variable.\n",
      "- \"results.outcome\": the outcome variable.\n",
      "- \"results.beta\": the beta variable.\n",
      "- \"results.units\": the units variable.\n",
      "- \"results.hazard_ratio\": the hazard ratio variable.\n",
      "- \"results.odds_ratio\": the odds ratio variable.\n",
      "- \"results.direction\": the direction variable.\n",
      "- \"results.SE\": the standard error of the estimate.\n",
      "- \"results.P-value\": the P-value for the estimate.\n",
      "- \"results.error\": an error message if the results are missing.\n",
      "- \"results.explanation\": an explanation of the results.\n",
      "\n",
      "The \"results.beta\" variable should be a numeric value, not a string. The \"results.units\" variable should be a numeric value, not a string. The \"results.hazard_ratio\" variable should be a numeric value, not a string. The \"results.odds_ratio\" variable should be a numeric value, not a string. The \"results.direction\" variable should be a string, not a numeric value. The \"results.SE\" variable should be a numeric value, not a string. The \"results.P-value\" variable should be a numeric value, not a string. The \"results.error\" variable should be a string, not a numeric value. The \"results.explanation\" variable should be a string, not a numeric value.\n"
     ]
    }
   ],
   "source": [
    "response = outputs[0][input_ids.shape[-1] :]\n",
    "result = tokenizer.decode(response, skip_special_tokens=True)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
